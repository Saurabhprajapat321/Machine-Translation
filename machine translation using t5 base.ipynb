{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Machine Translation using t5 base model using Hugging face library and hosted using Gradio Library \n"
      ],
      "metadata": {
        "id": "gQZY-VjqzU8q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Previously i have created a Language translation model using Encoder and Decoder model ,since encoder and decoder are base model and simple model and trained over small dataset due to which it cannot able to handle dynamic input \n",
        "\n",
        "So, i have created another machine translation model using t5 base model which is pretrained model and able to handle dynamic inputs Thanks to Hugging Face library and hosted it using Gradio Library "
      ],
      "metadata": {
        "id": "Xve7Y6DCzRn4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "Ni243pguykba"
      },
      "outputs": [],
      "source": [
        "!pip install -q gradio\n",
        "!pip install -q transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "95zlAv2XyoD-"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translation_pipeline = pipeline(\"translation_en_to_fr\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTFTC1cnypwE",
        "outputId": "398042e6-4d96-424c-f9ed-3431027e394a"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "No model was supplied, defaulted to t5-base and revision 686f1db (https://huggingface.co/t5-base).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "/usr/local/lib/python3.7/dist-packages/transformers/models/t5/tokenization_t5_fast.py:166: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
            "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
            "- Be aware that you SHOULD NOT rely on t5-base automatically truncating your input to 512 when padding/encoding.\n",
            "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
            "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
            "  FutureWarning,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "result = translation_pipeline(\"I like mathematics and computers\")"
      ],
      "metadata": {
        "id": "1I_W03UHyrrR"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate_gradio(input):\n",
        "    result = translation_pipeline(input)\n",
        "    return result[0]['translation_text']"
      ],
      "metadata": {
        "id": "HurrejIBywxS"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translate_interface = gr.Interface(fn = translate_gradio,\n",
        "                                   inputs=\"text\",\n",
        "                                   outputs=\"text\")"
      ],
      "metadata": {
        "id": "CyLWByeJyzVq"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translate_interface.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        },
        "id": "tl2DvPUGy2S_",
        "outputId": "6c17b6dc-530e-4ff2-d523-d6fb85c6d5b8"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. To show errors in colab notebook, set `debug=True` in `launch()`\n",
            "Running on public URL: https://19858.gradio.app\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting, check out Spaces: https://huggingface.co/spaces\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://19858.gradio.app\" width=\"900\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(<gradio.routes.App at 0x7f02c43a7f10>,\n",
              " 'http://127.0.0.1:7863/',\n",
              " 'https://19858.gradio.app')"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kaQVVCsKy4sr"
      },
      "execution_count": 31,
      "outputs": []
    }
  ]
}